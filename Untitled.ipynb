{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/Blackbak/Downloads/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/Blackbak/Downloads/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/Blackbak/Downloads/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/Blackbak/Downloads/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/Users/Blackbak/Downloads/mnist/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = mnist.train.images[:55000,:]\n",
    "y_train = mnist.train.labels[:55000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = y_train.reshape(len(y_train), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_discriminator_dimension = 28*28\n",
    "input_generator_dimension = 2\n",
    "data_placeholder = tf.placeholder(tf.float32, [None, input_discriminator_dimension])\n",
    "generator_input = tf.placeholder(tf.float32, [None, input_generator_dimension])\n",
    "labels = tf.placeholder(tf.float32, shape=[None, onehot_encoded.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def discriminator(data, labels, layers, activation, training=True):\n",
    "    with tf.variable_scope(\"discriminator\") as scope:\n",
    "        if not training:\n",
    "            scope.reuse_variables()\n",
    "        inputs = tf.concat(concat_dim=1, values=[data, labels])\n",
    "        Mi = inputs.shape[1]\n",
    "        ###weights and bias initialization\n",
    "        weights = {}\n",
    "        bias = {}\n",
    "        for layer, i in zip(layers, range(len(layers))):\n",
    "            Mo = layer\n",
    "            weights[\"weight_{}\".format(i)] = tf.get_variable(\"d_w_{}\".format(i),\n",
    "                                                initializer=tf.random_normal(dtype=tf.float32, shape=[Mi, Mo])) \n",
    "            bias[\"bias_{}\".format(i)] = tf.get_variable(\"d_b_{}\".format(i), initializer=tf.zeros(Mo))\n",
    "            Mi = Mo\n",
    "        weights[\"out\"] = tf.get_variable(\"d_w_out\", \n",
    "                                         initializer=tf.random_normal(shape=[Mo, 1], dtype=tf.float32))\n",
    "        bias[\"out\"] = tf.get_variable(\"d_b_out\", initializer=tf.zeros(1))\n",
    "        ###forward pass\n",
    "        layer_input = inputs\n",
    "        for layer in range(len(layers)):\n",
    "            layer_input = tf.layers.batch_normalization(layer_input, training=training)\n",
    "            layer_input = activation(tf.add(tf.matmul(layer_input, weights[\"weight_{}\".format(layer)]), \n",
    "                                            bias[\"bias_{}\".format(layer)]))\n",
    "            layer_input = tf.layers.dropout(layer_input, training=training)\n",
    "        logits = tf.add(tf.matmul(layer_input, weights[\"out\"]), bias[\"out\"])\n",
    "        layer_out = tf.nn.sigmoid(logits)\n",
    "    return layer_out, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generator(z, labels, output_dimension, layers, activation, training=True, reuse=False):\n",
    "    with tf.variable_scope(\"generator\") as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "        inputs = tf.concat(concat_dim=1, values=[z, labels])\n",
    "        Mi = inputs.shape[1]\n",
    "        weights = {}\n",
    "        bias = {}\n",
    "        for layer, i in zip(layers, range(len(layers))):\n",
    "            Mo = layer\n",
    "            weights[\"weight_{}\".format(i)] = tf.get_variable(\"g_w_{}\".format(i), \n",
    "                                                initializer=tf.random_normal(dtype=tf.float32, shape=[Mi, Mo]))\n",
    "            bias[\"bias_{}\".format(i)] = tf.get_variable(\"g_b_{}\".format(i), initializer=tf.zeros(Mo))\n",
    "            Mi = Mo\n",
    "        weights[\"out\"] = tf.get_variable(\"g_w_out\", \n",
    "                                         initializer=tf.random_normal(shape=[Mo, output_dimension], dtype=tf.float32))\n",
    "        bias[\"out\"] = tf.get_variable(\"g_b_out\", initializer=tf.zeros(output_dimension))\n",
    "        layer_input = inputs\n",
    "        for layer in range(len(layers)):\n",
    "            layer_input = tf.layers.batch_normalization(layer_input, training=training)\n",
    "            layer_input = activation(tf.add(tf.matmul(layer_input, weights[\"weight_{}\".format(layer)]), \n",
    "                                            bias[\"bias_{}\".format(layer)]))\n",
    "            layer_input = tf.layers.dropout(layer_input, training=training)\n",
    "        layer_out = tf.nn.sigmoid(tf.add(tf.matmul(layer_input, weights[\"out\"]), bias[\"out\"]))\n",
    "    return layer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = generator(generator_input, labels, output_dimension=input_discriminator_dimension, \n",
    "                layers=[10, 10], activation=tf.nn.leaky_relu)\n",
    "dis_data, dis_data_logits = discriminator(data_placeholder, labels, layers=[100, 100],\n",
    "                                          activation=tf.nn.leaky_relu)\n",
    "sampler = generator(generator_input, labels, output_dimension=input_discriminator_dimension, \n",
    "                    layers=[10, 10], activation=tf.nn.leaky_relu, training=False, reuse=True)\n",
    "dis_sample, dis_sample_logits = discriminator(gen, labels, layers=[100, 100], \n",
    "                                              activation=tf.nn.leaky_relu, reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
